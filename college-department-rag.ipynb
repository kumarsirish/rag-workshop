{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "582d11ec",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation) System\n",
    "## Non-existent Undergrad Department - DISE\n",
    "\n",
    "This notebook demonstrates building a basic RAG pipeline for question-answering about a fictional department (DISE).\n",
    "\n",
    "## What We'll Build:\n",
    "\n",
    "1. **Document Embedding** - Convert text documents into vector representations using `all-MiniLM-L6-v2`\n",
    "2. **Vector Index** - Create a FAISS index for fast similarity search\n",
    "3. **Retrieval** - Find relevant documents based on user queries using cosine similarity\n",
    "4. **Generation** - Use Llama 3.1-8B (via HuggingFace API) to generate answers from retrieved context\n",
    "\n",
    "## Key Technologies:\n",
    "- **SentenceTransformers**: Open-source embedding model\n",
    "- **FAISS**: Facebook's similarity search library\n",
    "- **HuggingFace Inference API**: Free LLM access without local model download\n",
    "\n",
    "## Simplifications:\n",
    "- No document chunking (documents are already small)\n",
    "- No authentication required (using free HF inference)\n",
    "- Minimal preprocessing for educational clarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fce352",
   "metadata": {},
   "source": [
    "## Setup Instructions (Optional)\n",
    "\n",
    "### Getting a HuggingFace Token:\n",
    "\n",
    "While this notebook uses HuggingFace's free inference API that works without authentication for some models, having a token provides better rate limits and access to more models.\n",
    "\n",
    "**Steps to get your free HF token:**\n",
    "\n",
    "1. Visit [https://huggingface.co/](https://huggingface.co/) and sign up (free)\n",
    "2. Go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "3. Click **\"New token\"** → Give it a name → Select **\"Read\"** access\n",
    "4. Copy the generated token\n",
    "5. Set it in your environment:\n",
    "   ```bash\n",
    "   export HF_TOKEN=\"your_token_here\"\n",
    "   ```\n",
    "   Or in Python:\n",
    "   ```python\n",
    "   import os\n",
    "   os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n",
    "   ```\n",
    "\n",
    "**Note**: This notebook currently works without a token for basic usage. Add one if you encounter rate limits or want access to gated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea893c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install faiss-cpu sentence-transformers \n",
    "import faiss #Facebook AI Similarity Search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "# Load open-source embedding model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"Loaded embedding model: all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bb31d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample documents to index\n",
    "\n",
    "fictious_department_info = [\n",
    "\"The Department of Intelligent Systems Engineering (DISE) is a small, focused department that works on applied AI and intelligent systems.\",\n",
    "\"It currently has around 40 students, with a healthy mix of undergraduate and postgraduate learners.\",\n",
    "\"The department is run by a team of 14 professors, including experienced faculty members and a few industry practitioners.\",\n",
    "\"Students can choose from about 5 courses, ranging from core subjects to electives and hands-on project work.\"\n",
    "\"Overall, DISE aims to prepare students for real-world engineering roles through practical learning and industry exposure.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e7521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate embeddings for the documents using sentence-transformers\n",
    "def generate_embeddings(documents):\n",
    "    embeddings = embedding_model.encode(documents, convert_to_numpy=True)\n",
    "    return embeddings.astype(\"float32\") #float32 array is required for FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ac75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = generate_embeddings(fictious_department_info)\n",
    "dimension = embeddings.shape[1]\n",
    "print(f\"Generated Embeddings: {embeddings}, Shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87dd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create FAISS index with ineer product metric - for cosine similarity\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(embeddings)\n",
    "print(f\"Number of documents indexed: {index.ntotal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dbd30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search FAISS index for top_k documents similar to the query\n",
    "def search_index(query, top_k=2):\n",
    "    query_embedding = generate_embeddings([query])\n",
    "    faiss.normalize_L2(query_embedding) # Normalize the query embedding to unit length\n",
    "    faiss.normalize_L2(embeddings) # Normalize the document embeddings to unit length\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    results = [(fictious_department_info[i], distances[0][idx]) for idx, i in enumerate(indices[0])]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888f964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Whats the full form of DISE?\"\n",
    "user_query = \"How many students in DISE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c556ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve documents based on the query\n",
    "retrieved_docs = search_index(user_query, top_k=2)\n",
    "print(\"Top retrieved documents:\")\n",
    "for doc, score in retrieved_docs:\n",
    "    print(f\"Document: {doc}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19108177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build prompt with retrieved documents\n",
    "def build_prompt(query, retrieved_docs):\n",
    "    context = \"\\n\".join([f\"- {doc}\" for doc, _ in retrieved_docs])\n",
    "    prompt = f\"Based on the following documents:\\n{context}\\nAnswer the question: {query}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build prompt\n",
    "user_prompt = build_prompt(user_query, retrieved_docs)\n",
    "print(f\"Constructed Prompt: {user_prompt}\")\n",
    "messages = [\n",
    "    #{\"role\": \"system\", \"content\": \"You are a helpful assistant. You should always answer\"},\n",
    "     {\"role\": \"system\", \"content\": \"You are a helpful assistant. Strictly use the provided documents to answer the user's question.\"},\n",
    "     {\"role\": \"user\", \"content\": user_prompt}\n",
    "   \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use HuggingFace Inference API (free, no auth required for some models)\n",
    "\n",
    "temperature = 0.8\n",
    "client = InferenceClient()\n",
    "response = client.chat_completion(\n",
    "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    messages=messages,\n",
    "    max_tokens=150,\n",
    "    temperature=temperature\n",
    ")\n",
    "answer = response.choices[0].message.content\n",
    "print(f\"Generated Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
