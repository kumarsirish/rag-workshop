{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarsirish/rag-workshop/blob/main/college-department-rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582d11ec",
      "metadata": {
        "id": "582d11ec"
      },
      "source": [
        "# RAG (Retrieval-Augmented Generation) System\n",
        "## Fictional Undergrad Department - DISE\n",
        "\n",
        "This notebook demonstrates building a basic RAG pipeline for question-answering about a fictional department (DISE).\n",
        "\n",
        "## What We'll Build:\n",
        "\n",
        "1. **Document Embedding** - Convert text documents into vector representations using `all-MiniLM-L6-v2`\n",
        "2. **Vector Index** - Create a FAISS index for fast similarity search\n",
        "3. **Retrieval** - Find relevant documents based on user queries using cosine similarity\n",
        "4. **Generation** - Use Llama 3.1-8B (via HuggingFace API) to generate answers from retrieved context\n",
        "\n",
        "## Key Technologies:\n",
        "- **SentenceTransformers**: Open-source embedding model\n",
        "- **FAISS**: Facebook's similarity search library\n",
        "- **HuggingFace Inference API**: Free LLM access without local model download\n",
        "\n",
        "## Simplifications:\n",
        "- No document chunking (documents are already small)\n",
        "- No authentication required (using free HF inference)\n",
        "- Minimal preprocessing for educational clarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19fce352",
      "metadata": {
        "id": "19fce352"
      },
      "source": [
        "## Setup Instructions (Optional)\n",
        "\n",
        "### Getting a HuggingFace Token:\n",
        "\n",
        "While this notebook uses HuggingFace's free inference API that works without authentication for some models, having a token provides better rate limits and access to more models.\n",
        "\n",
        "**Steps to get your free HF token:**\n",
        "\n",
        "1. Visit [https://huggingface.co/](https://huggingface.co/) and sign up (free)\n",
        "2. Go to [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "3. Click **\"New token\"** → Give it a name → Select **\"Read\"** access\n",
        "4. Copy the generated token\n",
        "5. Set it in your environment:\n",
        "   ```bash\n",
        "   export HF_TOKEN=\"your_token_here\"\n",
        "   ```\n",
        "   Or in Python:\n",
        "   ```python\n",
        "   import os\n",
        "   os.environ[\"HF_TOKEN\"] = \"your_token_here\"\n",
        "   ```\n",
        "   Or if running in Google Collab, then copy the token to the secrets with key as HF_TOKEN\n",
        "   \n",
        "\n",
        "**Note**: Add a token or you may encounter rate limits or want access to gated models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea893c2",
      "metadata": {
        "id": "9ea893c2"
      },
      "outputs": [],
      "source": [
        "! pip install faiss-cpu sentence-transformers\n",
        "import faiss #Facebook AI Similarity Search\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "\n",
        "# Load open-source embedding model\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(f\"Loaded embedding model: all-MiniLM-L6-v2\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13bb31d4",
      "metadata": {
        "id": "13bb31d4"
      },
      "outputs": [],
      "source": [
        "#Sample documents to index\n",
        "\n",
        "fictious_department_info = [\n",
        "\"The Department of Intelligent Systems Engineering (DISE) is a small, focused department that works on applied AI and intelligent systems.\",\n",
        "\"It currently has around 40 students, with a healthy mix of undergraduate and postgraduate learners.\",\n",
        "\"The department is run by a team of 14 professors, including experienced faculty members and a few industry practitioners.\",\n",
        "\"Students can choose from about 5 courses, ranging from core subjects to electives and hands-on project work.\",\n",
        "\"Overall, DISE aims to prepare students for real-world engineering roles through practical learning and industry exposure.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546e7521",
      "metadata": {
        "id": "546e7521"
      },
      "outputs": [],
      "source": [
        "#Generate embeddings for the documents using sentence-transformers\n",
        "def generate_embeddings(documents):\n",
        "    embeddings = embedding_model.encode(documents, convert_to_numpy=True)\n",
        "    return embeddings.astype(\"float32\") #float32 array is required for FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8ac75f",
      "metadata": {
        "id": "7d8ac75f"
      },
      "outputs": [],
      "source": [
        "embeddings = generate_embeddings(fictious_department_info)\n",
        "dimension = embeddings.shape[1]\n",
        "print(f\"Generated Embeddings: {embeddings}, Shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d87dd86",
      "metadata": {
        "id": "5d87dd86"
      },
      "outputs": [],
      "source": [
        "#Create FAISS index with ineer product metric - for cosine similarity\n",
        "faiss.normalize_L2(embeddings)\n",
        "index = faiss.IndexFlatIP(dimension)\n",
        "index.add(embeddings)\n",
        "print(f\"Number of documents indexed: {index.ntotal}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dbd30e",
      "metadata": {
        "id": "f1dbd30e"
      },
      "outputs": [],
      "source": [
        "# Search FAISS index for top_k documents similar to the query\n",
        "def search_index(query, top_k=2):\n",
        "    query_embedding = generate_embeddings([query])\n",
        "    faiss.normalize_L2(query_embedding) # Normalize the query embedding to unit length\n",
        "    distances, indices = index.search(query_embedding, top_k)\n",
        "\n",
        "    results = []\n",
        "    for idx, i in enumerate(indices[0]):\n",
        "        results.append((fictious_department_info[i], distances[0][idx]))\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "888f964a",
      "metadata": {
        "id": "888f964a"
      },
      "outputs": [],
      "source": [
        "user_query = \"Whats the full form of DISE?\"\n",
        "#user_query = \"How many students in DISE\"\n",
        "#user_query = \"how many casual leaves can i get\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07c556ad",
      "metadata": {
        "id": "07c556ad"
      },
      "outputs": [],
      "source": [
        "#retrieve documents based on the query\n",
        "retrieved_docs = search_index(user_query, top_k=2)\n",
        "print(\"Top retrieved documents:\")\n",
        "for doc, score in retrieved_docs:\n",
        "    print(f\"Document: {doc}, Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19108177",
      "metadata": {
        "id": "19108177"
      },
      "outputs": [],
      "source": [
        "#Build prompt with retrieved documents\n",
        "def build_prompt(query, retrieved_docs):\n",
        "    context_lines = []\n",
        "    for doc, _ in retrieved_docs:\n",
        "        context_lines.append(f\"- {doc}\")\n",
        "    context = \"\\n\".join(context_lines)\n",
        "    prompt = f\"Based on the following documents:\\n{context}\\nAnswer the question: {query}\"\n",
        "    # Removed the line that was overwriting the prompt with a simpler version:\n",
        "    # prompt = f\"Answer the question : {query}\"\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f23e7aa",
      "metadata": {
        "id": "6f23e7aa"
      },
      "outputs": [],
      "source": [
        "# Build prompt\n",
        "user_prompt = build_prompt(user_query, retrieved_docs)\n",
        "rag_prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You must always answer the question asked\"},\n",
        "    # {\"role\": \"system\", \"content\": \"You are a helpful assistant. Strictly use the provided documents to answer the user's question.\"},\n",
        "     {\"role\": \"user\", \"content\": user_prompt}\n",
        "\n",
        "]\n",
        "\n",
        "simple_prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. You must always answer\"},\n",
        "      {\"role\": \"user\", \"content\": user_query}\n",
        "\n",
        "]\n",
        "\n",
        "print(f\"Constructed Prompt: {rag_prompt}\")\n",
        "print(f\"Simple prompt: {simple_prompt}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
        "<br><b>\n",
        "The size of the model in terms of parameters is 8B.  '8B' means it has 8 billion parameters. This is considered a relatively small model compared to larger versions (like 70B or even larger proprietary models), making it efficient for deployment on consumer hardware or for tasks where speed and lower resource usage are critical, while still offering good capabilities.\n",
        "</b>"
      ],
      "metadata": {
        "id": "2sQ7Y687w9QP"
      },
      "id": "2sQ7Y687w9QP"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(prompt):\n",
        "  response = client.chat_completion(\n",
        "    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    messages=prompt,\n",
        "    max_tokens=150,\n",
        "    temperature=0.9\n",
        "  )\n",
        "  answer = response.choices[0].message.content\n",
        "  return answer"
      ],
      "metadata": {
        "id": "plhqqelRUqc8"
      },
      "id": "plhqqelRUqc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14db64fe",
      "metadata": {
        "id": "14db64fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Use HuggingFace Inference API (free, no auth required for some models)\n",
        "\n",
        "#hf_token = userdata.get('HF_TOKEN').strip()\n",
        "\n",
        "#client = InferenceClient(token=hf_token)\n",
        "client = InferenceClient()\n",
        "response = get_response(rag_prompt)\n",
        "print(f\"Generated Answer with RAG: {response}\")\n",
        "\n",
        "response = get_response(simple_prompt)\n",
        "print(f\"\\nGenerated Answer without RAG: {response}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}