{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kumarsirish/rag-workshop/blob/main/scholarships-rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c883708c",
      "metadata": {
        "id": "c883708c"
      },
      "source": [
        "# RAG Implementation Chatbot to get information about the scholarships provided by Govt. of India\n",
        "## https://scholarships.gov.in/\n",
        "\n",
        "\n",
        "In this notebook we will build a simple RAG application based on a scholraship dataset from Govt. of India. It has following sections\n",
        "* Load the dataset\n",
        "* Chunking - Splitting of the Data\n",
        "* Vector Database\n",
        "* Generating Embedding\n",
        "* Encoding user query, Creating the prompt and generating the similarity score\n",
        "* Generate the output and channel it through the LLM for the proper response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76bdd295"
      },
      "source": [
        "\n",
        "### Key Components and Workflow:\n",
        "\n",
        "1.  **Dataset Loading**: The project utilizes a dataset of Indian government scholarships, sourced from scholarships.gov.in and made available on Hugging Face (`NetraVerse/indian-govt-scholarships`).\n",
        "\n",
        "2.  **Data Chunking**: The raw text data from the scholarships is split into smaller, manageable chunks to improve the relevance and efficiency of information retrieval.\n",
        "\n",
        "3.  **Vector Database**: [Qdrant](https://qdrant.tech/) is employed as the vector database to store and manage the embeddings of these document chunks.\n",
        "\n",
        "4.  **Embedding Generation**: [SentenceTransformer](https://www.sbert.net/) (`all-MiniLM-L6-v2`) is used to convert both the scholarship document chunks and user queries into dense vector representations (embeddings). This model maps sentences and paragraphs into a 384-dimensional vector space, enabling semantic similarity search.\n",
        "\n",
        "5.  **Retrieval**: When a user poses a query, its embedding is generated and used to search the Qdrant vector database. The system retrieves the most semantically similar document chunks to the query.\n",
        "\n",
        "6.  **Language Model (LLM)**: [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) is the chosen Large Language Model. It's a smaller, efficient model capable of generating coherent responses.\n",
        "\n",
        "7.  **Response Generation (RAG)**: The retrieved document chunks are provided as context to TinyLlama. The LLM then generates a factual and relevant answer to the user's query, grounded in the provided scholarship information.\n",
        "\n",
        "8.  **Interactive Interface**: The entire RAG pipeline is encapsulated within a [Gradio](https://www.gradio.app/) interface, allowing users to interact with the chatbot in real-time."
      ],
      "id": "76bdd295"
    },
    {
      "cell_type": "markdown",
      "id": "e6add53f",
      "metadata": {
        "id": "e6add53f"
      },
      "source": [
        "### Load the Dataset.\n",
        "Govt. of India data is available at https://scholarships.gov.in/ which was uploaded to the Hugging Face. Uploading to the Hugging face is already done.\n",
        "In this cell we would download the dataset from HF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01642a8d",
      "metadata": {
        "id": "01642a8d"
      },
      "outputs": [],
      "source": [
        "! pip install pandas\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Read scholarship data from parquet file\n",
        "df = pd.read_parquet(\"hf://datasets/NetraVerse/indian-govt-scholarships/data/train-00000-of-00001.parquet\")\n",
        "df = df[['label', 'text']]\n",
        "\n",
        "# Convert to dict format\n",
        "data = df.to_dict('records')\n",
        "print(f\"Loaded {len(data)} scholarship documents\")\n",
        "pprint(data[:1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25dd4a13",
      "metadata": {
        "id": "25dd4a13"
      },
      "source": [
        "### Chunk the Data - Splitting into smaller pieces\n",
        "* We will split the data into smaller chunks to make it easier to process and retrieve relevant information.\n",
        "* Interactive chunking experience is available at https://chunkviz.up.railway.app/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c99df7b",
      "metadata": {
        "id": "4c99df7b"
      },
      "outputs": [],
      "source": [
        "# Set this to True to enable chunking, False to disable\n",
        "ENABLE_CHUNKING = True\n",
        "\n",
        "def chunk_text(text, chunk_size, overlap):\n",
        "    '''Split text into overlapping chunks'''\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = start + chunk_size\n",
        "        chunk = text[start:end]\n",
        "        chunks.append(chunk)\n",
        "        start += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "if ENABLE_CHUNKING:\n",
        "    # Create chunked version of data\n",
        "    chunked_data = []\n",
        "    for doc in data:\n",
        "        text = doc['text']  #data under 'text' key\n",
        "        chunks = chunk_text(text, chunk_size=500, overlap=100)  #500 characters with 100 characters overlap\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            chunked_data.append({\n",
        "                'label': doc['label'],\n",
        "                'text': chunk,\n",
        "                'chunk_id': i,\n",
        "                'total_chunks': len(chunks)\n",
        "            })\n",
        "\n",
        "    # Reassign data with chunked data\n",
        "    data = chunked_data\n",
        "\n",
        "    print(f\"CHUNKING ENABLED\")\n",
        "    print(f\"Chunked into {len(data)} pieces\")\n",
        "    # Display first chunk example - FULL TEXT\n",
        "    print(\"FIRST CHUNK EXAMPLE:\")\n",
        "    print(f\"Chunk ID: {data[0]['chunk_id']} of {data[0]['total_chunks']}\")\n",
        "else:\n",
        "    print(f\"CHUNKING DISABLED - Using full documents\")\n",
        "    print(f\"Total documents: {len(data)}\")\n",
        "    print(\"FIRST DOCUMENT EXAMPLE:\")\n",
        "\n",
        "print(f\"Label: {data[0]['label']}\")\n",
        "print(f\"Text Length: {len(data[0]['text'])} characters\")\n",
        "print(f\"FULL TEXT:\\n{data[0]['text']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169aa59e",
      "metadata": {
        "id": "169aa59e"
      },
      "source": [
        "### üì¶ Install required dependencies for vector database, embeddings, and deep learning\n",
        "* Vector database used is qdrant\n",
        "* Embeddings model is from sentence transformers. This maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search. https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
        "* Deep learning model is from Hugging Face.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283b7db8",
      "metadata": {
        "id": "283b7db8"
      },
      "outputs": [],
      "source": [
        "! pip install qdrant-client\n",
        "! pip install sentence-transformers\n",
        "! pip install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8615e2c0",
      "metadata": {
        "id": "8615e2c0"
      },
      "source": [
        "### üì¶ Initialize Qdrant vector database client and SentenceTransformer embedding encoder\n",
        "* Vector database is used to store and retrieve document chunks based on their semantic similarity to the query.\n",
        "* SentenceTransformer is used to convert text into dense vector representations (embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eeb69f0",
      "metadata": {
        "id": "3eeb69f0"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import models, QdrantClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
        "\n",
        "encoder_model = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b29fe9ce",
      "metadata": {
        "id": "b29fe9ce"
      },
      "source": [
        "###  üì¶ Initialize vector database for storing scholarship embeddings with cosine similarity. Other similarity functions are\n",
        "* DOT Product (models.Distance.DOT)\n",
        "* Euclidean (models.Distance.EUCLIDIAN)\n",
        "* Manhattan (models.Distance.MANHATTAN)\n",
        "* ...etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20e2c2a",
      "metadata": {
        "id": "c20e2c2a"
      },
      "outputs": [],
      "source": [
        "# Create collection to store the scholarship data\n",
        "collection_name=\"scholarships\"\n",
        "\n",
        "qdrant.recreate_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=models.VectorParams(\n",
        "        size=encoder_model.get_sentence_embedding_dimension(), # Vector size is as defined in the used model\n",
        "        distance=models.Distance.COSINE\n",
        "    )\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0a18dcf",
      "metadata": {
        "id": "b0a18dcf"
      },
      "source": [
        "### üì¶ Generate embeddings for each document and upload to vector database\n",
        "* In vector database, each data point is represented as a vector in a high-dimensional space.\n",
        "* For all-MiniLM-L6-v2 model, each vector has 384 dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4e97d02",
      "metadata": {
        "id": "d4e97d02"
      },
      "outputs": [],
      "source": [
        "points_to_upload = []\n",
        "for idx, doc in enumerate(data):\n",
        "    points_to_upload.append(\n",
        "        models.PointStruct(\n",
        "            id=idx,\n",
        "            vector=encoder_model.encode(doc[\"text\"]).tolist(),  # Use 'text' field for scholarship data\n",
        "            payload=doc\n",
        "        )\n",
        "    )\n",
        "\n",
        "# vectorize and upload points to Qdrant\n",
        "qdrant.upload_points(\n",
        "    collection_name=collection_name,\n",
        "    points=points_to_upload\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c93270fa",
      "metadata": {
        "id": "c93270fa"
      },
      "source": [
        "### üì¶ Check the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7fdd160",
      "metadata": {
        "id": "b7fdd160"
      },
      "outputs": [],
      "source": [
        "# Display first document's text and embedding\n",
        "first_doc = data[0]\n",
        "first_text = first_doc['text']\n",
        "first_vector = encoder_model.encode(first_text).tolist()\n",
        "\n",
        "print(\"DOCUMENT TEXT:\")\n",
        "print(f\"Text (first 100 chars): {first_text[:100]}...\")\n",
        "print(\"EMBEDDING VECTOR:\")\n",
        "print(f\"Vector dimension: {len(first_vector)}\")\n",
        "print(f\"First 20 values: {first_vector[:20]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "096fbced"
      },
      "source": [
        "### Check the number of points (embeddings) in the collection in qdrant"
      ],
      "id": "096fbced"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "010dbe18"
      },
      "source": [
        "count_result = qdrant.count(collection_name=collection_name, exact=True)\n",
        "print(f\"Number of points in collection '{collection_name}': {count_result.count}\")"
      ],
      "id": "010dbe18",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa64a646"
      },
      "source": [
        "### Retrieve and display a specific point (embedding and its payload)\n",
        "\n",
        "You can retrieve a point by its `id`. For example, let's look at the point with `id=0` (which corresponds to the first chunk of data)."
      ],
      "id": "fa64a646"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df3a9ce0"
      },
      "source": [
        "point_id = 0\n",
        "retrieved_point = qdrant.retrieve(collection_name=collection_name, ids=[point_id], with_vectors=True, with_payload=True)\n",
        "\n",
        "if retrieved_point:\n",
        "    print(f\"--- Retrieved Point ID: {retrieved_point[0].id} ---\")\n",
        "    print(f\"Payload: \")\n",
        "    pprint(retrieved_point[0].payload)\n",
        "    print(f\"Vector (first 2 values): {retrieved_point[0].vector[:2]}...\")\n",
        "    print(f\"Vector dimension: {len(retrieved_point[0].vector)}\")\n",
        "else:\n",
        "    print(f\"Point with ID {point_id} not found.\")"
      ],
      "id": "df3a9ce0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "bbe6d3e0",
      "metadata": {
        "id": "bbe6d3e0"
      },
      "source": [
        "### üì¶ User query and searching the database\n",
        " * Define user query\n",
        " * Convert user query to embedding using the same SentenceTransformer model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54c23040",
      "metadata": {
        "id": "54c23040"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"what is the percetnage reservations for women in NSPG Scheme\"\n",
        "#SentenceTransoformer model returns a NumPy array or PyTorch Tensor but qdrant\n",
        "# expects in the list format.\n",
        "query_vector = encoder_model.encode(user_prompt).tolist()\n",
        "print(f\"query_vector: {query_vector}\")\n",
        "print(f\"Query Vector Dimension: {len(query_vector)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99b4ab6b",
      "metadata": {
        "id": "99b4ab6b"
      },
      "source": [
        "### üéØ Search vector database\n",
        "* Search the vector database for the top 3 (top k) most similar document chunks based on cosine similarity.\n",
        "* Display the retrieved document chunks with metadata and similarity scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e74206db",
      "metadata": {
        "id": "e74206db"
      },
      "outputs": [],
      "source": [
        "# Search time for awesome wines!\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import SearchParams, ScoredPoint\n",
        "\n",
        "hits = qdrant.query_points(\n",
        "    collection_name=collection_name,\n",
        "    query=query_vector,\n",
        "    limit=3\n",
        ")\n",
        "\n",
        "#  save the search results\n",
        "search_results = []\n",
        "for hit in hits.points:\n",
        "    search_results.append(hit.payload)\n",
        "    pprint(hit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09621e9e",
      "metadata": {
        "id": "09621e9e"
      },
      "source": [
        "### ü§ñ Load TinyLlama model\n",
        "* TinyLlama is a smaller version of the LLaMA model, designed to be more efficient while still providing good performance for various NLP tasks.\n",
        "* We will use TinyLlama to generate responses based on the retrieved document chunks.\n",
        "* https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e96276d8",
      "metadata": {
        "id": "e96276d8"
      },
      "outputs": [],
      "source": [
        "# For Hugging Face models\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "# Set up device (GPU if available, else CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load TinyLlama model and tokenizer\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c18e428",
      "metadata": {
        "id": "2c18e428"
      },
      "source": [
        "### ü§ñ Generate response using TinyLlama without search results\n",
        "* Generate a response to the user query using TinyLlama without incorporating any retrieved document chunks.\n",
        "* This serves as a baseline to compare against the RAG approach.\n",
        "* Return Tensor output is of PyTorch type.\n",
        "* max_new_tokens: The maximum number of new tokens to generate in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edc36c19",
      "metadata": {
        "id": "edc36c19"
      },
      "outputs": [],
      "source": [
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot. Your top priority is to help users and guide them with their queries. \"},\n",
        "    {\"role\": \"user\",\"content\": user_prompt},\n",
        "]\n",
        "\n",
        "print(prompt)\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tprompt,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\", #pt, np, tf\n",
        ").to(model.device)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
        "pprint(\"Response without RAG and with TinyLlama:\")\n",
        "pprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b19537",
      "metadata": {
        "id": "33b19537"
      },
      "source": [
        "### ‚ú® Generate response using TinyLlama WITH search results (RAG-ENHANCED)\n",
        "* Generate a response to the user query using TinyLlama  incorporating retrieved document chunks.\n",
        "* max_new_tokens: The maximum number of new tokens to generate in the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1388103a",
      "metadata": {
        "id": "1388103a"
      },
      "outputs": [],
      "source": [
        "# No need to reload the model - just create a new prompt with RAG context\n",
        "\n",
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": f\"You are a helpful chatbot specializing in Indian government scholarships. Use the following retrieved documents to answer the user's question accurately.ONLY use information from the retrieved documents.\\n\\nRetrieved Documents:\\n{str(search_results)}\"},\n",
        "    {\"role\": \"user\", \"content\": user_prompt},\n",
        "]\n",
        "\n",
        "print(prompt)\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "\tprompt,\n",
        "\tadd_generation_prompt=True,\n",
        "\ttokenize=True,\n",
        "\treturn_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ").to(model.device)\n",
        "pprint(\"Response with  RAG and with TinyLlama:\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=500)\n",
        "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
        "pprint(response)\n",
        "\n",
        "# Print source documents in one line\n",
        "sources = \" | \".join([f\"{doc['label']}\" for doc in search_results])\n",
        "print(f\"\\nüìö Sources: {sources}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baa8cc1b",
      "metadata": {
        "id": "baa8cc1b"
      },
      "source": [
        "<h4>üåê Launch interactive Gradio chatbot interface with full RAG pipeline</h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e1a927",
      "metadata": {
        "id": "40e1a927"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def scholarship_chatbot(message, history):\n",
        "    # Encode user query\n",
        "    query_vector = encoder_model.encode(message).tolist()\n",
        "\n",
        "    # Search for relevant scholarships\n",
        "    hits = qdrant.query_points(\n",
        "        collection_name=collection_name,\n",
        "        query=query_vector,\n",
        "        limit=3\n",
        "    )\n",
        "\n",
        "    search_results = []\n",
        "    for hit in hits.points:\n",
        "        search_results.append(hit.payload)\n",
        "\n",
        "    # Generate response with LLM\n",
        "    prompt = [\n",
        "        {\"role\": \"system\", \"content\": f\"You are a helpful chatbot specializing in Indian government scholarships. Use the following retrieved documents to answer accurately:\\n\\nRetrieved Documents:\\n{str(search_results)}\"},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        prompt,\n",
        "        add_generation_prompt=True,\n",
        "        tokenize=True,\n",
        "        return_dict=True,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens=1024)\n",
        "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
        "\n",
        "    # Add source documents in one line\n",
        "    sources_list = []\n",
        "    for doc in search_results:\n",
        "        sources_list.append(f\"{doc['label']}\")\n",
        "    sources = \" | \".join(sources_list)\n",
        "    response_with_sources = f\"{response}\\n\\n Sources: {sources}\"\n",
        "\n",
        "    return response_with_sources\n",
        "\n",
        "# Launch Gradio interface\n",
        "demo = gr.ChatInterface(\n",
        "    scholarship_chatbot,\n",
        "    title=\"üéì Indian Government Scholarship Chatbot\",\n",
        "    description=\"Ask me about Indian government scholarships!\",\n",
        "    examples=[\n",
        "        \"What scholarships are available for engineering students?\",\n",
        "        \"Tell me about AICTE scholarships\",\n",
        "        \"Are there scholarships for women in STEM? Summarize the answer\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}