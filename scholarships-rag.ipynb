{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c883708c",
   "metadata": {},
   "source": [
    "# RAG Implementation Chatbot to get information about the scholarships provided by Govt. of India\n",
    "\n",
    "\n",
    "In this notebook we will build a simple RAG application based on a scholraship dataset from Govt. of India. It has following sections\n",
    "* Load the dataset\n",
    "* Chunking - Splitting of the Data\n",
    "* Vector Database\n",
    "* Generating Embedding\n",
    "* Encoding user query, Creating the prompt and generating the similarity score\n",
    "* Generate the output and channel it through the LLM for the proper response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6add53f",
   "metadata": {},
   "source": [
    "### Load the Dataset. \n",
    "Govt. of India data is available at https://scholarships.gov.in/ which was uploaded to the Hugging Face. Uploading to the Hugging face is already done.\n",
    "In this cell we would download the dataset from HF. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01642a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Read scholarship data from parquet file\n",
    "df = pd.read_parquet(\"hf://datasets/NetraVerse/indian-govt-scholarships/data/train-00000-of-00001.parquet\")\n",
    "df = df[['label', 'text']]\n",
    "\n",
    "# Convert to records format\n",
    "data = df.to_dict('records')\n",
    "print(f\"Loaded {len(data)} scholarship documents\")\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25dd4a13",
   "metadata": {},
   "source": [
    "### Chunk the Data - Splitting into smaller pieces\n",
    "* We will split the data into smaller chunks to make it easier to process and retrieve relevant information.\n",
    "* Interactive chunking experience is available at https://chunkviz.up.railway.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c99df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to True to enable chunking, False to disable\n",
    "ENABLE_CHUNKING = True \n",
    "\n",
    "def chunk_text(text, chunk_size, overlap):\n",
    "    '''Split text into overlapping chunks'''\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "if ENABLE_CHUNKING:\n",
    "    # Create chunked version of data\n",
    "    chunked_data = []\n",
    "    for doc in data:\n",
    "        text = doc['text']  #data under 'text' key\n",
    "        chunks = chunk_text(text, chunk_size=500, overlap=100)  #500 characters with 100 characters overlap\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunked_data.append({\n",
    "                'label': doc['label'],\n",
    "                'text': chunk,\n",
    "                'chunk_id': i,\n",
    "                'total_chunks': len(chunks)\n",
    "            })\n",
    "    \n",
    "    # Reassign data with chunked data\n",
    "    data = chunked_data\n",
    "    \n",
    "    print(f\"CHUNKING ENABLED\")\n",
    "    print(f\"Chunked into {len(data)} pieces\")\n",
    "    # Display first chunk example - FULL TEXT\n",
    "    print(\"FIRST CHUNK EXAMPLE:\")\n",
    "    print(f\"Chunk ID: {data[0]['chunk_id']} of {data[0]['total_chunks']}\")\n",
    "else:\n",
    "    print(f\"CHUNKING DISABLED - Using full documents\")\n",
    "    print(f\"Total documents: {len(data)}\")\n",
    "    print(\"FIRST DOCUMENT EXAMPLE:\")\n",
    "    \n",
    "print(f\"Label: {data[0]['label']}\")\n",
    "print(f\"Text Length: {len(data[0]['text'])} characters\")\n",
    "print(f\"FULL TEXT:\\n{data[0]['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169aa59e",
   "metadata": {},
   "source": [
    "### üì¶ Install required dependencies for vector database, embeddings, and deep learning\n",
    "* Vector database used is qdrant\n",
    "* Embeddings model is from sentence transformers. This maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search. https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2\n",
    "* Deep learning model is from Hugging Face.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283b7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install qdrant-client\n",
    "! pip install sentence-transformers\n",
    "! pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615e2c0",
   "metadata": {},
   "source": [
    "### üì¶ Initialize Qdrant vector database client and SentenceTransformer embedding encoder\n",
    "* Vector database is used to store and retrieve document chunks based on their semantic similarity to the query.\n",
    "* SentenceTransformer is used to convert text into dense vector representations (embeddings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eeb69f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "qdrant = QdrantClient(\":memory:\") # Create in-memory Qdrant instance\n",
    "\n",
    "encoder = SentenceTransformer('all-MiniLM-L6-v2') # Model to create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29fe9ce",
   "metadata": {},
   "source": [
    "###  üì¶ Initialize vector database for storing scholarship embeddings with cosine similarity. Other similarity functions are\n",
    "* DOT Product (models.Distance.DOT)\n",
    "* Euclidean (models.Distance.EUCLIDIAN)\n",
    "* Manhattan (models.Distance.MANHATTAN)\n",
    "* ...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e2c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collection to store the scholarship data\n",
    "collection_name=\"scholarships\"\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(), # Vector size is as defined in the used model\n",
    "        distance=models.Distance.COSINE \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a18dcf",
   "metadata": {},
   "source": [
    "### üì¶ Generate embeddings for each document and upload to vector database\n",
    "* In vector database, each data point is represented as a vector in a high-dimensional space.\n",
    "* For all-MiniLM-L6-v2 model, each vector has 384 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e97d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_to_upload = []\n",
    "for idx, doc in enumerate(data):\n",
    "    points_to_upload.append(\n",
    "        models.PointStruct(\n",
    "            id=idx,\n",
    "            vector=encoder.encode(doc[\"text\"]).tolist(),  # Use 'text' field for scholarship data\n",
    "            payload=doc\n",
    "        )\n",
    "    )\n",
    "\n",
    "# vectorize and upload points to Qdrant\n",
    "qdrant.upload_points(\n",
    "    collection_name=collection_name,\n",
    "    points=points_to_upload\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93270fa",
   "metadata": {},
   "source": [
    "### üì¶ Check the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fdd160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first document's text and embedding\n",
    "first_doc = data[0]\n",
    "first_text = first_doc['text']\n",
    "first_vector = encoder.encode(first_text).tolist()\n",
    "\n",
    "print(\"DOCUMENT TEXT:\")\n",
    "print(f\"Text (first 100 chars): {first_text[:100]}...\")\n",
    "print(\"EMBEDDING VECTOR:\")\n",
    "print(f\"Vector dimension: {len(first_vector)}\")\n",
    "print(f\"First 20 values: {first_vector[:20]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6d3e0",
   "metadata": {},
   "source": [
    "### üì¶ User query and searching the database\n",
    " * Define user query \n",
    " * Convert user query to embedding using the same SentenceTransformer model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c23040",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = \"what is the percetnage reservations for women in NSPG Scheme\"\n",
    "query_vector = encoder.encode(user_prompt).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b4ab6b",
   "metadata": {},
   "source": [
    "### üéØ Search vector database\n",
    "* Search the vector database for the top 3 (top k) most similar document chunks based on cosine similarity.\n",
    "* Display the retrieved document chunks with metadata and similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74206db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search time for awesome wines!\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import SearchParams, ScoredPoint\n",
    "\n",
    "hits = qdrant.query_points(\n",
    "    collection_name=collection_name,\n",
    "    query=query_vector,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "#  hold the search results\n",
    "search_results = [hit.payload for hit in hits.points]\n",
    "\n",
    "for hit in hits.points: # Print the search hits\n",
    "  pprint(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09621e9e",
   "metadata": {},
   "source": [
    "### ü§ñ Load TinyLlama model\n",
    "* TinyLlama is a smaller version of the LLaMA model, designed to be more efficient while still providing good performance for various NLP tasks.\n",
    "* We will use TinyLlama to generate responses based on the retrieved document chunks.\n",
    "* https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96276d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Hugging Face models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set up device (GPU if available, else CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load TinyLlama model and tokenizer\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18e428",
   "metadata": {},
   "source": [
    "### ü§ñ Generate response using TinyLlama without search results\n",
    "* Generate a response to the user query using TinyLlama without incorporating any retrieved document chunks.\n",
    "* This serves as a baseline to compare against the RAG approach.\n",
    "* Return Tensor output is of PyTorch type.\n",
    "* max_new_tokens: The maximum number of new tokens to generate in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc36c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot specializing in Indian government scholarships. Your top priority is to help users find relevant scholarship information and guide them with their queries. ONLY use information from the retrieved documents\"},\n",
    "    {\"role\": \"user\",\"content\": user_prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tprompt,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=1024)\n",
    "pprint(\"Response without RAG and with TinyLlama:\")\n",
    "pprint(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b19537",
   "metadata": {},
   "source": [
    "### ‚ú® Generate response using TinyLlama WITH search results (RAG-ENHANCED)\n",
    "* Generate a response to the user query using TinyLlama  incorporating retrieved document chunks.\n",
    "* max_new_tokens: The maximum number of new tokens to generate in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to reload the model - just create a new prompt with RAG context\n",
    "\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": f\"You are a helpful chatbot specializing in Indian government scholarships. Use the following retrieved documents to answer the user's question accurately.ONLY use information from the retrieved documents.\\n\\nRetrieved Documents:\\n{str(search_results)}\"},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tprompt,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "pprint(\"Response with  RAG and with TinyLlama:\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "pprint(response)\n",
    "\n",
    "# Print source documents in one line\n",
    "sources = \" | \".join([f\"{doc['label']}\" for doc in search_results])\n",
    "print(f\"\\nüìö Sources: {sources}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8cc1b",
   "metadata": {},
   "source": [
    "<h4>üåê Launch interactive Gradio chatbot interface with full RAG pipeline</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e1a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def scholarship_chatbot(message, history):\n",
    "    # Encode user query\n",
    "    query_vector = encoder.encode(message).tolist()\n",
    "    \n",
    "    # Search for relevant scholarships\n",
    "    hits = qdrant.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=3\n",
    "    )\n",
    "    \n",
    "    search_results = [hit.payload for hit in hits.points]\n",
    "    \n",
    "    # Generate response with LLM\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a helpful chatbot specializing in Indian government scholarships. Use the following retrieved documents to answer accurately:\\n\\nRetrieved Documents:\\n{str(search_results)}\"},\n",
    "        {\"role\": \"user\", \"content\": message}\n",
    "    ]\n",
    "    \n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=True,\n",
    "        return_dict=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024)\n",
    "    response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:])\n",
    "    \n",
    "    # Add source documents in one line\n",
    "    sources = \" | \".join([f\"{doc['label']}\" for doc in search_results])\n",
    "    response_with_sources = f\"{response}\\n\\n Sources: {sources}\"\n",
    "    \n",
    "    return response_with_sources\n",
    "\n",
    "# Launch Gradio interface\n",
    "demo = gr.ChatInterface(\n",
    "    scholarship_chatbot,\n",
    "    title=\"üéì Indian Government Scholarship Chatbot\",\n",
    "    description=\"Ask me about Indian government scholarships!\",\n",
    "    examples=[\n",
    "        \"What scholarships are available for engineering students?\",\n",
    "        \"Tell me about AICTE scholarships\",\n",
    "        \"Are there scholarships for women in STEM?\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
